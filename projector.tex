\chapter{Projector for orthogonal subspaces}

\section{Direct sum}
% From Meyer book
In general given two subspaces \( \mathcal{U} \) and \( \mathcal{Y} \) of \( \mathbb{R}^n \),
such that \( \mathcal{U} \oplus \mathcal{Y} = \mathcal{V} \) we have that
$$ \forall v \in \mathcal{V} \quad \exists! u \in \mathcal{U} \quad \exists! y \in \mathcal{Y} \quad v = u + y $$

$u$ is the projection of $v$ onto $\mathcal{U}$ along $\mathcal{Y}$ and $y$ is the projection of $v$ onto $\mathcal{Y}$ along $\mathcal{U}$.

% we can find a projector \( P \) onto \( \mathcal{U} \) along \( \mathcal{V} \) by the following procedure:
\section{Projector}
Given two subspaces $\mathcal{X}, \mathcal{Y}$ such that $\mathcal{X} \oplus \mathcal{Y} =V$.
$ \forall v \in \mathcal{V} \quad v = x + y \quad x \in \mathcal{X}, y \in \mathcal{Y}$.
We know $v$ and we would like to compute $x$. We can construct a projection matrix $P$ called projector such that
$$ Pv = x $$

We know the basis for $X$ and $Y$.
$$ \mathcal{X} = span\{x_1, x_2, \ldots, x_r\} \quad \mathcal{Y} = span\{y_1, y_2, \ldots, y_{m - r}\} $$

we can construct a matrix $B$ such that:
$$ B = (x_1, x_2, \ldots, x_r, y_1, y_2, \ldots, y_{m - r}), \quad V = R(B) $$

$B$ is non singular (full rank) and we can compute its inverse $B^{-1}$.
\[
\begin{aligned}
    B &= (X, Y) \quad Pv = x
    \Rightarrow P(x + y) &= Px + Py = x \\
    PB &= P(X, Y) = (PX, PY) = (X, 0) \\
    P &= PBB^{-1} = (X, 0)B^{-1} \\
    (X, 0) &= B\begin{pmatrix}
        I & 0 \\
        0 & 0
    \end{pmatrix} = (X, Y)\begin{pmatrix}
        I & 0 \\
        0 & 0
    \end{pmatrix} \\
    P &= B\begin{pmatrix}
        I & 0 \\
        0 & 0
    \end{pmatrix}B^{-1}
\end{aligned}
\]

To compute the projector onto $\mathcal{Y}$ along $\mathcal{X}$ if we know $P$
\[
Pv = x \Rightarrow v = Iv = (P + (I - P))v = Pv + (I - P)v = x + (I - P)v = x + y
\]

$I - P$ is the projector onto $\mathcal{Y}$ along $\mathcal{X}$.

\[
\begin{aligned}
    I - P &= BB^{-1} - B\begin{pmatrix}
        I & 0 \\
        0 & 0
    \end{pmatrix}B^{-1} = B(I - \begin{pmatrix}
        I & 0 \\
        0 & 0
    \end{pmatrix})B^{-1} \\
    I - P &= B\begin{pmatrix}
        0 & 0 \\
        0 & I
    \end{pmatrix}B^{-1}
\end{aligned}
\]
\section{Properties of the projector}
\begin{enumerate}
    \item The projector is unique
    \item The projector is idempotent
    \item $I - P$ is the complementary projector
    \item The range of $P$ is equal to the range of $X$ which is equal to the subspace $\mathcal{X}$
        \[ R(P) = \{ x | Pv = x \} = \mathcal{X} \]
    \item The range of $P$ is equal to the null space of $I - P$.
\[N(I - P) = \mathcal{X} \quad (I - P)y = y \quad (I - P)x = 0 \]
\end{enumerate}

\section{Orthogonal projector}
$M \subseteq V$ and its orthogonal complementary subspace of $M^{\perp}$, $M \oplus M^{\perp} = V$.
The inner product between $x \in M, y \in M^{\perp}$ is zero
$$x^Ty = 0 \quad \forall x \in M, y \in M^{\perp}$$

if $m_1, m_2, \ldots, m_r$ is a basis for $M$ and $N=M^{\perp}$, then $n_1, n_2, \ldots, n_s$ is a basis for $N$
$$ m_j^Tn_i = 0 \quad \text{for } i=1,2,\ldots,s \text{ and } j=1,2,\ldots,r $$

The projector $P_M$ onto $M$ along $N$ is called \textbf{orthogonal projector}.

$$
M = (m_1, m_2, \ldots, m_r) \quad B = (M, N) \quad N = (n_1, n_2, \ldots, n_s)
$$

$$
M^T - N = 0 \quad N^T M = 0
$$
$$
P_M = B\begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix}B^{-1} = (M, N)\begin{pmatrix}
I & 0 \\
0 & 0
\end{pmatrix}\begin{pmatrix}
M & N
\end{pmatrix}^{-1}
$$
If we consider the situation where $m_1, m_2, \ldots, m_r$, $n_1, n_2, \ldots, n_s$ are orthonormal basis for $M$ and $N$ respectively, then
$ M^TM = I$, ($MM^T \neq I$) and $N^TN = I$, ($NN^T \neq I$).
$$
\begin{pmatrix}
    M & N
\end{pmatrix}^{-1} = \begin{pmatrix}
    (M^TM)^{-1}M^T \\
    N^T
\end{pmatrix} = \begin{pmatrix}
    M^T \\
    N^T
\end{pmatrix}
$$
$$
P_M = \begin{pmatrix}
    M & N
\end{pmatrix}\begin{pmatrix}
    I & 0 \\
    0 & 0
\end{pmatrix}\begin{pmatrix}
    M^T \\
    N^T
\end{pmatrix} = \begin{pmatrix}
    M & N
\end{pmatrix}\begin{pmatrix}
    M^T \\
    0
\end{pmatrix} = MM^T
$$

Let $A \in \mathbb{R}^{m \times n}$ be a matrix of maximum rank $n$ then all the columns of $A$ are linearly independent.
We want to compute $P_{R(A)}$ the orthogonal projector onto $R(A)$ along $N(A^T)$. We know that $R(A) \oplus N(A^T) = \mathbb{R}^m$ so $R(A)^{\perp} = N(A^T)$.
$$ P_{R(A)} = (M, N)\begin{pmatrix}
    I & 0 \\
    0 & 0
    \end{pmatrix}\begin{pmatrix}
        (M^TM)^{-1}M^T \\
        N^T
    \end{pmatrix} = M(M^TM)^{-1}M^T $$

Since a basis for $R(A)$ are the columns of $A$.
$$ P_{R(A)} = M(M^TM)^{-1}M^T = A(A^TA)^{-1}A^T$$

We found this expression $(A^TA)^{-1}A^T$ when introducing normal equation and solution of least square problem.
We show the relation between the projector and the least square problem.

For a solution of the linear system we would like to minimize the 2-norm of the residual $r = A\hat{x} - b$.
We choose $\hat{x}$ such that $r \perp R(A)$, $r \in N(A^T)$.
$$
\begin{aligned}
    A^T(A\hat{x} - b) = 0 \\
    A^TA\hat{x} - A^Tb = 0 \\
    A^TA\hat{x} = A^Tb \implies \hat{x} = (A^TA)^{-1}A^Tb    
\end{aligned}
$$
we look now for the projection of b in the range of A


$$
\begin{aligned}
Ax &= P_{R(A)}b \\
Ax &= A(A^TA)^{-1}A^Tb \\
Ax &= A(A^TA)^{-1}A^Tb = P_{R(A)}b
\end{aligned}
$$
where $(A^TA)^{-1}A^T$ is the pseudo inverse of A

Example

$$
x = \begin{pmatrix}
    \frac{1}{\sqrt{3}} \\
    \frac{1}{\sqrt{3}} \\
    \frac{1}{\sqrt{3}}
\end{pmatrix} \quad
x^Tx = 1
$$
$$
M = \begin{pmatrix}
    \frac{1}{\sqrt{3}} \\
    \frac{1}{\sqrt{3}} \\
    \frac{1}{\sqrt{3}}
\end{pmatrix} \quad M = span\{x\} \quad M \oplus M^T = 0
$$


\section{Computation of an orthogonal projector using the SVD}
$$
A = U\begin{pmatrix}
    \Sigma_1 & 0 \\
    0 & 0
\end{pmatrix}V^T
= (U_A U_{A^{\perp}})\begin{pmatrix}
    \Sigma_1 & 0 \\
    0 & 0
\end{pmatrix}
\begin{pmatrix}
 V_A \\
 V_{A^{\perp}}
\end{pmatrix}
$$

The columsn of $U_A$ are the orthonormal basis of $R(A)$
and the columns of $U_{A^{\perp}}$ are the orthonormal basis of $N(A^T)$

$$
R(A) \oplus N(A^T) = \mathbb{R}^m \quad R(A) \perp N(A^T)
$$ $$
P_{R(A)} = U_AU_A^T \quad P_{R(A^T)} = V_AV_A^T
$$ $$
P_{N(A^T)} = I - U_AU_A^T \quad P_{N(A)} = I - V_AV_A^T
$$
Consider a vector $v \in \mathbb{R}^m$ and its projection $P_{R(A)}v$ onto $R(A)$
$$
P_{R(A)}v = U_AU_A^Tv = U_A(U_A^Tv) = U_A \cdot z \quad z = U_A^Tv
$$

An useful application of the projection is that we can compare vectors in the form $z = U_A^Tv$ instead of $v$ or $P_{R(A)}v$
taking advantage of the reduced dimension of $z$.

\section{Term document matrix}
Let $A \in \mathbb{R}^{m \times n}$ be a matrix with $m >> n$ and its QR factorization with column pivoting:
$$ AP = Q\begin{pmatrix}
    R \\ O
\end{pmatrix} = (Q_A, Q_{A^{\perp}})\begin{pmatrix}
    R_1 & R_2 \\ O & 0
\end{pmatrix}
$$

$$ P_{R(A)} = Q_AQ_A^T \quad P_{N(A^T)} = I - P_{R(A)}$$

Let $q \in R^{m}$ be a query than $$q = P_{R(A)}q + (I-P_{R(A)})q = q_A + q_{A^\perp}$$

$ q_A = Q_AQ_A^Tq $ is the orthogonal projection of $q$ onto $R(A)$ along $R(A)^{\perp}$
% P_{R(A^T)} = Q_{A^{\perp}}Q_{A^{\perp}}^T

\section{Closest point theorem}
Let $M$ be a subspace of a inner product space $V$ and let $b \in V$.
Then there exists a unique vector that is closest to b is $P_Mb$ and it is the orthogonal projection of $b$ onto $M$.

$$ \min_{\substack{b \in V \\ m \in M}} ||b - m|| = ||b - P_Mb|| $$
Suppose $q$ is our query
$$ q_A = Q_AQ_A^Tq = P_A q  \quad \|q - q_A \|_2 \leq \|q - x \|_2 \quad \forall x \in R(A) $$

$$
\begin{aligned}
    \|q - x \|_2^2 &= \|(q - q_A) + (q_A - x) \|_2^2 \\
    &= (q - q_A)^T(q - q_A) + (q_A - x)^T(q_A - x) + (q - q_A)^T(q_A - x) + (q_A - x)^T(q - q_A)\\
    &= \|q - q_A \|_2^2 + \|q_A - x \|_2^2 + (q - q_A)^T(q_A - x) + (q_A - x)^T(q - q_A) \quad q_{A^{\perp}} = q - q_A \\
    &= \|q - q_A \|_2^2 + \|q_A - x \|_2^2 + + \underbrace{q_{A^\perp}^T(q_A - x)}_{0} + \underbrace{(q_A - x)^Tq_{A^\perp}}_{0}\\
    &= \|q - q_A \|_2^2 + \|q_A - x \|_2^2 \geq \|q - q_A \|_2^2
\end{aligned}
$$

In the analaysis of text documents, we compute
$$ cos(v_j) = \frac{a_j^Tq}{\|a_j\|_2 \|q\|_2} $$
$$ A = (a_1, a_2, \ldots, a_n) \quad a_1, a_2, \ldots, a_n \text{ are documents} $$
$$ cos(v_j) = \frac{a_j^T(q_A + q_{A^{\perp}})}{\|a_j\|_2 \|q\|_2} = 
\frac{a_j^Tq_A + \overbrace{a_j^Tq_{A^{\perp}}}^{=0}}{\|a_j\|_2 \|q\|_2}
$$

If we use only $q_A$ we can compute
$$ cos(\bar{\theta_j}) = \frac{a_j^Tq_A}{\|a_j\|_2 \|q_A\|_2}$$

this allows to recover more relevant documents, but increase the number of irrelevant ones $ \|q_A\|_2 \leq \|q\|_2$.

\section{Latent Semantic Indexing}
In the latent Semantic Indexing (LSI), the matrix A is approximated using only $k$ singular vectors and
$k$ singular values.

$$ A = \sum_{i=1}^r \sigma_i u_i v_i^T $$
We choose $k < r$ and compute
$$\sum_{i=1}^k \sigma_i u_i v_i^T = A_k = U_k\Sigma_kV_k^T$$
$$U_k = (u_1, u_2, \ldots, u_k) \quad V_k = (v_1, v_2, \ldots, v_k) \quad \Sigma_k = diag(\sigma_1, \sigma_2, \ldots, \sigma_k)$$

We can choose $k$ such that
$$ \frac{\|A - A_k\|}{\|A\|} \leq \epsilon $$

where $\epsilon$ is a small number, our tolerance.
We compare our query vector with the columns of $A_k ~ A$ we compute the $cos(\bar{\theta_j})$ using the factors $U_k, V_k, \Sigma_k$.

$$
cos(\theta^k_j) = \frac{(A_ke_j)^Tq}{\|(A_ke_j)\|_2 \|q\|_2}
= \frac{(U_k\Sigma_k V_k^T e_j)^Tq}{\|U_k\Sigma_k V_k^T e_j\|_2 \|q\|_2}
= \frac{(U_k\Sigma_k V_k^T e_j)^Tq}{\|\Sigma_k V_k^T e_j\|_2 \|q\|_2}
= \frac{v_j^T\Sigma_k^Tu_k^Tq}{\|\Sigma_k^Tu_k^Tq\|_2}$$