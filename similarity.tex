\chapter{Similarity}

Given a matrix \( A \) of size  $m \times m$, it would be useful to transform it into
another matrix of some structure (triangular or diagonal) with the same spectrum.
In particular, we would like to construct what is called a similar matrix.


\textbf{Def.} Given matrices \( A, B \in \mathbb{R}^{m \times m} \), they are said to be \textit{similar} if there exists a non-singular matrix \( S \) such that
\[
S^{-1}AS = B.
\]
This is called a \textbf{similarity transformation}.


\textbf{Proof:} We see that \( B \) has the same spectrum as \( A \). 
Let's say \( \lambda \) be an eigenvalue of \( A \) with \( Ax = \lambda x \) and \( x \neq 0 \). Then,
\begin{align*}
    S^{-1}Ax &= \lambda S^{-1}x \\
    S^{-1}AIx &= \lambda S^{-1}x \\
    S^{-1}A(SS^{-1})x &= \lambda S^{-1}x \\
    (S^{-1}AS)(S^{-1}x) & = \lambda (S^{-1}x) \\
    B(S^{-1}x) & = \lambda (S^{-1}x)
\end{align*}
Setting \( y := S^{-1}x \), we get $By = \lambda y$, which shows \( \lambda \) is an eigenvalue of \( B \) as well.


\textbf{Proposition:} If \( A \in \mathbb{R}^{m \times m} \) has \( m \) linearly independent eigenvectors, then the matrix \( S \) composed of these eigenvectors as columns is such that
\[
S^{-1}AS = \Lambda = \begin{pmatrix}
    \lambda_1 &  & 0 \\
     & \ddots &  \\
    0 &  & \lambda_n
    \end{pmatrix}
\]
where \( \Lambda \) is a diagonal matrix with the eigenvalues of \( A \) on the diagonal.

\textbf{Proof:} Let \( S = \begin{pmatrix} x_1 & x_2 & \dots & x_m \end{pmatrix} \), where \( x_i \) are eigenvectors of \( A \) corresponding to eigenvalues \( \lambda_i \). Then
\[
AS = A\begin{pmatrix} x_1 & x_2 & \dots & x_m \end{pmatrix}
= \begin{pmatrix} Ax_1 &  Ax_2 & \dots &  Ax_m \end{pmatrix}
= \begin{pmatrix} \lambda_1 x_1 & \lambda_2 x_2 & \dots & \lambda_m x_m \end{pmatrix}
= S \Lambda
\]
Thus,
\[
S^{-1}AS = \Lambda.
\]
\textbf{Observation:}
\begin{itemize}
    \item The matrix \( S \) is not unique since we could consider multiples of any eigenvector.
    \item If a matrix \( A \) $n \times n$ has \( n \) distinct eigenvalues, then it has \( n \) linearly independent eigenvectors.
\end{itemize}

\section{Algebraic and Geometric Multiplicity}
\subsection{Algebraic multiplicity}
The \textit{algebraic multiplicity} of a given eigenvalue is the number of times that eigenvalue is a root of the associated characteristic polynomial.

Consider, for example, the polynomial $p(\lambda) = (\lambda - 1)^2$. To compute the roots of $p(\lambda)$:
\begin{align*}
p(\lambda) &= 0 \\
(\lambda - 1)^2 &= 0 \\
(\lambda - 1)(\lambda - 1) &= 0
\end{align*}
This implies that $\lambda = 1$ with algebraic multiplicity equal to $t$.
\textbf{Example:} 
For the matrix 
\[
A = 
\begin{pmatrix}
3 & 1 & 2 \\
0 & 3 & 4 \\
0 & 0 & 3 
\end{pmatrix}
\quad
\lambda = 3 \text{ with algebraic multiplicity} = 3
\]

\subsection{Geometric multiplicity}
The \textit{geometric multiplicity} of a given eigenvalue is the dimension of the associated eigenspace: $$dim(\mathcal{N}(A - \lambda I))$$
\paragraph{Example:}
For the matrix 
\[
A = 
\begin{pmatrix}
3 & 1 \\
0 & 3 
\end{pmatrix}
\]
$(A - \lambda I) = 
\begin{pmatrix}
0 & 1 \\
0 & 0 
\end{pmatrix}
$ and $\lambda = 3$ with algebraic multiplicity $= 2$.

Thus, 
\[
\mathcal{N}
\left[\begin{pmatrix}
0 & 1 \\
0 & 0 
\end{pmatrix}\right]
= \text{span} 
\left\{ \begin{pmatrix}
0 \\
1 
\end{pmatrix} \right\}
\]
and the geometric multiplicity of $\lambda$ is $1$.

In general, the geometric multiplicity $\leq$ algebraic multiplicity.
When the geometric multiplicity $=$ algebraic multiplicity, then the eigenvalues
are said to be \textit{semi-simple}. When the eigenvalues are all distinct,
they are said to be \textit{simple}. We can show that if a matrix $A$ has 
either distinct or semi-distinct eigenvalues, then it has a set of linearly
independent eigenvectors.

\section*{Diagonalizability}

\textbf{Question:} When is a given matrix \( A \) similar to a diagonal matrix?

\textbf{Answer:} If the matrix \( A \) has a complete set of eigenvectors, then it's said to be diagonalizable.

A matrix \( A \) has a complete set of eigenvectors if the eigenvectors are linearly independent.

For example, if the given matrix \( A \) does not have a complete set of eigenvectors, then \( A \) is said to be defective.

We cannot diagonalize defective matrices.

For example, consider the matrix
\[
A = \begin{pmatrix}
3 & 1 \\
0 & 3 
\end{pmatrix}
\]
\( \lambda = 3 \) with algebraic multiplicity \( = 2 \) and \( \ker(A - \lambda I) = \) span \( \left\{ \begin{pmatrix} 1 \\ 0 \end{pmatrix} \right\} \), only one linearly independent eigenvector. Therefore, \( A \) is not diagonalizable.

Diagonalizability is not the hard to achieve in practice and there are several numerical methods that can be used. However, when asking if a given matrix \( A \) could be similar to a diagonal matrix, we ask if it could be similar to a triangular matrix.



\section*{Schur's Triangularization Theorem}

Every square matrix is unitarily similar to an upper triangular matrix.

This means that there exist a matrix \( Q \) such that 
\[ Q^*AQ = T \]
with \( T \) upper triangular.

Note: \( Q \in \mathbb{C} \) is unitary if \( Q^*Q = I \).

\textbf{Theorem: Real Schur Canonical Form}

Given a matrix \( A \) with real coefficients there exists an orthogonal matrix \( Q \) with real coefficients such that
\[ Q^T AQ = T \]
with \( T \) quasi-upper triangular.

This means that the diagonal elements of \( T \) are either \( 1 \times 1 \) blocks or \( 2 \times 2 \) blocks with complex-conjugate eigenvalues.

\section{Application to Spectral Clustering}

Given a dataset \( X \) of points we would like to cluster them using "meaningful groups". Our aim is to keep similar clustering algorithms based on some "distance" measure, but unfortunately, when the set is not convex for example, those algorithms are prone to fail. Hence we need to resort to a different kind of clustering such as spectral clustering.

This means we need to find a more "global" method of representation.

\subsection{Similarity Graph}

To model the local neighborhood relations between data points in \( X \) as a graph where
\textbf{nodes} are the data points and \textbf{edges} are the connections between the points.
If the \textbf{distance} between node \( i \) and node \( j \) is natural, then we have an 
edge between node \( i \) and node \( j \). The edges are weighted by \textbf{similarity}:
\[ s_{ij} = \exp\left(-\frac{dist(i,j)}{\sigma}\right)^2 \]

where \( dist(i,j) \) is the distance between node \( i \) and node \( j \).

We collect \( s_{ij} \) into a matrix \( S \) which is the adjacency matrix of the constructed graph.
\begin{mdframed}
    Note:
    \begin{itemize}
        \item This matrix \( S \) is symmetric.
        \item If any \( S_{ij} = 0 \) then node \( i \) and node \( j \) are not connected.
    \end{itemize}
\end{mdframed}
    
% Degree matrix D
We define the degree matrix \( D \) as a diagonal matrix such that the diagonal 
entry \( D_{ii} \) is the sum of the weights of the edges attached to vertex \( i \),
that is,
$$ D_{ii} = \sum_{j=1}^{n} s_{ij}. $$

\subsection{Laplacian Matrix}

\begin{itemize}
    \item The Laplacian matrix \( L := D - S \)
    \item The normalized Laplacian is given by $L_{u} = D^{-1} L$
    \item The normalized symmetric Laplacian $ L_s = D^{-1/2} S D^{-1/2}$
\end{itemize}


\subsection{Computing Eigenvectors}

% Computing eigenvectors
To compute the eigenvectors and eigenvalues of the Laplacian matrix. In particular,
the number of eigenvalues equal to zero will tell us how many clusters we should create.
Then, taking exactly the same number of eigenvectors, we have a more representative and
different points. This means that in the new representation, the given data in \( X \)
will be "separeted". We can apply a spectral clustering algorithm based on the new basis vectors.

Please have a look at the code \texttt{SpectralClustExample.m} in the Matlab folder.
